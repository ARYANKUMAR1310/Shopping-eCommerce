{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-01T16:36:24.934829Z","iopub.execute_input":"2024-08-01T16:36:24.935711Z","iopub.status.idle":"2024-08-01T16:36:25.373020Z","shell.execute_reply.started":"2024-08-01T16:36:24.935643Z","shell.execute_reply":"2024-08-01T16:36:25.371965Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/online-shoppers-purchase-intention/online_shoppers_intention.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **INPUT**","metadata":{}},{"cell_type":"code","source":"shoppers_data=pd.read_csv(\"/kaggle/input/online-shoppers-purchase-intention/online_shoppers_intention.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T16:36:28.801026Z","iopub.execute_input":"2024-08-01T16:36:28.801549Z","iopub.status.idle":"2024-08-01T16:36:28.873713Z","shell.execute_reply.started":"2024-08-01T16:36:28.801515Z","shell.execute_reply":"2024-08-01T16:36:28.872452Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# **PREPROCESSING**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nimport numpy as np\nimport pandas as pd\n\nshoppers_data['Session_Length'] = shoppers_data['Administrative_Duration'] + shoppers_data['Informational_Duration'] + shoppers_data['ProductRelated_Duration']\n# Create 'Visited_All_Three'\nshoppers_data['Visited_All_Three'] = ((shoppers_data['Administrative'] > 0) & (shoppers_data['Informational'] > 0) & (shoppers_data['ProductRelated'] > 0)).astype(int)\n\n# One-hot encode 'VisitorType'\none_hot_encoder = OneHotEncoder(sparse=False)\nvisitor_type_encoded = one_hot_encoder.fit_transform(shoppers_data[['VisitorType']])\nvisitor_type_df = pd.DataFrame(visitor_type_encoded, columns=one_hot_encoder.get_feature_names_out(['VisitorType']))\nshoppers_data = pd.concat([shoppers_data, visitor_type_df], axis=1)\nshoppers_data.drop('VisitorType', axis=1, inplace=True)\n\n# Cyclical encoding for 'Month'\nmonths = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'June': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\nshoppers_data['Month'] = shoppers_data['Month'].map(months)\nshoppers_data['Month_Sin'] = np.sin(2 * np.pi * shoppers_data['Month']/12)\nshoppers_data['Month_Cos'] = np.cos(2 * np.pi * shoppers_data['Month']/12)\nshoppers_data.drop('Month', axis=1, inplace=True)\n\n# Encode 'Weekend' and 'Revenue'\nshoppers_data['Weekend'] = shoppers_data['Weekend'].astype(int)\nshoppers_data['Revenue'] = shoppers_data['Revenue'].astype(int)\n\n# Now you can save or further process the modified DataFrame\n","metadata":{"execution":{"iopub.status.busy":"2024-08-01T16:36:43.393433Z","iopub.execute_input":"2024-08-01T16:36:43.393836Z","iopub.status.idle":"2024-08-01T16:36:43.959124Z","shell.execute_reply.started":"2024-08-01T16:36:43.393805Z","shell.execute_reply":"2024-08-01T16:36:43.958067Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Hard Voted Ensemble ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix, roc_auc_score\nfrom category_encoders import TargetEncoder\nfrom imblearn.over_sampling import SVMSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom lightgbm import LGBMClassifier\nimport warnings\nimport numpy as np\n\n# Ignore FutureWarnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Assuming 'shoppers_data' is already loaded and is your DataFrame\n# Identify numerical columns\nnumerical_columns = ['Administrative', 'Informational', 'ProductRelated', 'Administrative_Duration', 'Informational_Duration', 'ProductRelated_Duration', 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay','Month_Sin','Month_Cos','Session_Length']\n\n# Preparing the data\nX = shoppers_data.drop('Revenue', axis=1)\ny = shoppers_data['Revenue']\n\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the RobustScaler\nscaler = RobustScaler()\nX_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\nX_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n\n# Apply SVMSMOTE for oversampling\nsvmsmote = SVMSMOTE(sampling_strategy=0.6, random_state=42)\nX_resampled, y_resampled = svmsmote.fit_resample(X_train, y_train)\n\n# Apply RandomUnderSampler for undersampling to 80% of the majority class size\nrus = RandomUnderSampler(sampling_strategy=0.8, random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)\n\n# Configure AdaBoost\nada_boost = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators=180,\n    learning_rate=0.1,\n    random_state=1\n)\n\n# Configure LightGBM\nlgbm = LGBMClassifier(\n    boosting_type='dart',\n    colsample_bytree=0.8,\n    learning_rate=0.05,\n    max_depth=7,\n    n_estimator=150,\n    num_leaves=31,\n    subsample=0.8,\n    subsample_freq=1,\n    min_child_weight=0.001,\n    reg_alpha=0.1,\n    reg_lambda=0.1,\n    random_state=12\n)\n\n# Ensemble: Soft Voting\nensemble = VotingClassifier(\n    estimators=[('ada', ada_boost), ('lgbm', lgbm)],\n    voting='hard'\n)\n\n# Encoding categorical features\nencoder = TargetEncoder(cols=['OperatingSystems', 'Browser', 'Region', 'TrafficType'], smoothing=50, min_samples_leaf=10)\nX_resampled = encoder.fit_transform(X_resampled, y_resampled)\nX_test_encoded = encoder.transform(X_test)\n\n# Fit the ensemble on the resampled training data\nensemble.fit(X_resampled, y_resampled)\n\n# Predicting and evaluating on the original (non-resampled) test set\npredictions = ensemble.predict(X_test_encoded)\n#probs = ensemble.predict_proba(X_test_encoded)[:, 1]  # Probability estimates needed for AUC\nreport_dict = classification_report(y_test, predictions, output_dict=True)\nreport_df = pd.DataFrame(report_dict).transpose()\nreport_df['precision'] = report_df['precision'].apply(lambda x: format(x, '.4f'))\nreport_df['recall'] = report_df['recall'].apply(lambda x: format(x, '.4f'))\nreport_df['f1-score'] = report_df['f1-score'].apply(lambda x: format(x, '.4f'))\nreport_df['support'] = report_df['support'].apply(lambda x: int(x))\n\n# Print the formatted classification report\nprint(\"Classification Report with Controlled Decimal Places\")\nprint(report_df)\n\n# Calculate and print F1 Score for the Positive Class\nf1 = f1_score(y_test, predictions, pos_label=1)\nprint(f\"F1 Score for the Positive Class: {f1:.4f}\")\n\n# Compute AUC score\n#auc_score = roc_auc_score(y_test, probs)\n#print(f\"AUC Score: {auc_score:.4f}\")\n\n# Compute and print the confusion matrix\ncm = confusion_matrix(y_test, predictions)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Calculate TPR, FPR, etc from the confusion matrix\ntn, fp, fn, tp = cm.ravel()\ntpr = tp / (tp + fn)  # True Positive Rate\nfpr = fp / (fp + tn)  # False Positive Rate\ntnr = tn / (tn + fp)  # True Negative Rate\nfnr = fn / (tp + fn)  # False Negative Rate\n\nprint(f\"True Positive Rate (TPR): {tpr:.4f}\")\nprint(f\"False Positive Rate (FPR): {fpr:.4f}\")\nprint(f\"True Negative Rate (TNR): {tnr:.4f}\")\nprint(f\"False Negative Rate (FNR): {fnr:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-01T16:42:08.361981Z","iopub.execute_input":"2024-08-01T16:42:08.362741Z","iopub.status.idle":"2024-08-01T16:42:16.811467Z","shell.execute_reply.started":"2024-08-01T16:42:08.362703Z","shell.execute_reply":"2024-08-01T16:42:16.810109Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 5020, number of negative: 6275\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004293 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3746\n[LightGBM] [Info] Number of data points in the train set: 11295, number of used features: 22\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444444 -> initscore=-0.223144\n[LightGBM] [Info] Start training from score -0.223144\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nClassification Report with Controlled Decimal Places\n             precision  recall f1-score  support\n0               0.9381  0.9299   0.9340     2055\n1               0.6643  0.6934   0.6786      411\naccuracy        0.8905  0.8905   0.8905        0\nmacro avg       0.8012  0.8117   0.8063     2466\nweighted avg    0.8925  0.8905   0.8914     2466\nF1 Score for the Positive Class: 0.6786\nConfusion Matrix:\n[[1911  144]\n [ 126  285]]\nTrue Positive Rate (TPR): 0.6934\nFalse Positive Rate (FPR): 0.0701\nTrue Negative Rate (TNR): 0.9299\nFalse Negative Rate (FNR): 0.3066\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Soft Voted Ensemble\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix, roc_auc_score\nfrom category_encoders import TargetEncoder\nfrom imblearn.over_sampling import SVMSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom lightgbm import LGBMClassifier\nimport warnings\nimport numpy as np\n\n# Ignore FutureWarnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Assuming 'shoppers_data' is already loaded and is your DataFrame\n# Identify numerical columns\nnumerical_columns = ['Administrative', 'Informational', 'ProductRelated', 'Administrative_Duration', 'Informational_Duration', 'ProductRelated_Duration', 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay','Month_Sin','Month_Cos','Session_Length']\n\n# Preparing the data\nX = shoppers_data.drop('Revenue', axis=1)\ny = shoppers_data['Revenue']\n\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the RobustScaler\nscaler = RobustScaler()\nX_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\nX_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n\n# Apply SVMSMOTE for oversampling\nsvmsmote = SVMSMOTE(sampling_strategy=0.6, random_state=42)\nX_resampled, y_resampled = svmsmote.fit_resample(X_train, y_train)\n\n# Apply RandomUnderSampler for undersampling to 80% of the majority class size\nrus = RandomUnderSampler(sampling_strategy=0.8, random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)\n\n# Configure AdaBoost\nada_boost = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators=180,\n    learning_rate=0.1,\n    random_state=1\n)\n\n# Configure LightGBM\nlgbm = LGBMClassifier(\n    boosting_type='dart',\n    colsample_bytree=0.8,\n    learning_rate=0.05,\n    max_depth=7,\n    n_estimator=150,\n    num_leaves=31,\n    subsample=0.8,\n    subsample_freq=1,\n    min_child_weight=0.001,\n    reg_alpha=0.1,\n    reg_lambda=0.1,\n    random_state=12\n)\n\n# Ensemble: Soft Voting\nensemble = VotingClassifier(\n    estimators=[('ada', ada_boost), ('lgbm', lgbm)],\n    voting='soft'\n)\n\n# Encoding categorical features\nencoder = TargetEncoder(cols=['OperatingSystems', 'Browser', 'Region', 'TrafficType'], smoothing=50, min_samples_leaf=10)\nX_resampled = encoder.fit_transform(X_resampled, y_resampled)\nX_test_encoded = encoder.transform(X_test)\n\n# Fit the ensemble on the resampled training data\nensemble.fit(X_resampled, y_resampled)\n\n# Predicting and evaluating on the original (non-resampled) test set\npredictions = ensemble.predict(X_test_encoded)\nprobs = ensemble.predict_proba(X_test_encoded)[:, 1]  # Probability estimates needed for AUC\nreport_dict = classification_report(y_test, predictions, output_dict=True)\nreport_df = pd.DataFrame(report_dict).transpose()\nreport_df['precision'] = report_df['precision'].apply(lambda x: format(x, '.4f'))\nreport_df['recall'] = report_df['recall'].apply(lambda x: format(x, '.4f'))\nreport_df['f1-score'] = report_df['f1-score'].apply(lambda x: format(x, '.4f'))\nreport_df['support'] = report_df['support'].apply(lambda x: int(x))\n\n# Print the formatted classification report\nprint(\"Classification Report with Controlled Decimal Places\")\nprint(report_df)\n\n# Calculate and print F1 Score for the Positive Class\nf1 = f1_score(y_test, predictions, pos_label=1)\nprint(f\"F1 Score for the Positive Class: {f1:.4f}\")\n\n# Compute AUC score\nauc_score = roc_auc_score(y_test, probs)\nprint(f\"AUC Score: {auc_score:.4f}\")\n\n# Compute and print the confusion matrix\ncm = confusion_matrix(y_test, predictions)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Calculate TPR, FPR, etc from the confusion matrix\ntn, fp, fn, tp = cm.ravel()\ntpr = tp / (tp + fn)  # True Positive Rate\nfpr = fp / (fp + tn)  # False Positive Rate\ntnr = tn / (tn + fp)  # True Negative Rate\nfnr = fn / (tp + fn)  # False Negative Rate\n\nprint(f\"True Positive Rate (TPR): {tpr:.4f}\")\nprint(f\"False Positive Rate (FPR): {fpr:.4f}\")\nprint(f\"True Negative Rate (TNR): {tnr:.4f}\")\nprint(f\"False Negative Rate (FNR): {fnr:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-01T07:47:21.690370Z","iopub.execute_input":"2024-08-01T07:47:21.691160Z","iopub.status.idle":"2024-08-01T07:47:30.265288Z","shell.execute_reply.started":"2024-08-01T07:47:21.691124Z","shell.execute_reply":"2024-08-01T07:47:30.264267Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 5020, number of negative: 6275\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004457 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3746\n[LightGBM] [Info] Number of data points in the train set: 11295, number of used features: 22\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444444 -> initscore=-0.223144\n[LightGBM] [Info] Start training from score -0.223144\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nClassification Report with Controlled Decimal Places\n             precision  recall f1-score  support\n0               0.9566  0.9119   0.9337     2055\n1               0.6430  0.7932   0.7102      411\naccuracy        0.8921  0.8921   0.8921        0\nmacro avg       0.7998  0.8526   0.8220     2466\nweighted avg    0.9043  0.8921   0.8965     2466\nF1 Score for the Positive Class: 0.7102\nAUC Score: 0.9325\nConfusion Matrix:\n[[1874  181]\n [  85  326]]\nTrue Positive Rate (TPR): 0.7932\nFalse Positive Rate (FPR): 0.0881\nTrue Negative Rate (TNR): 0.9119\nFalse Negative Rate (FNR): 0.2068\n","output_type":"stream"}],"execution_count":7}]}